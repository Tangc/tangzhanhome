# AI原生应用架构的运行时、可观测、评估、安全内容总结。

大家好，我是唐斩，这是第70天第84篇分享。
## 1、AI原生应用架构-运行时、可观测、评估、安全篇
阿里云《AI原生应用白皮书》运行时部分的内容记录
- 传统 serverless 架构面向确定性、无状态、短周期HTTP请求，不符合动态性、有状态、长周期复杂交互的 Agent 场景
- AI原生应用运行时需要的核心能力是
  - 会话级状态管理和安全隔离(有虚拟运行代码能力时需要硬隔离)
  - 实时弹性和成本核算(毫秒伸缩、实际用量计费)
  - 异构算力和工具连接(cpu、gpu资源分别核算，mcp、a2a连接)
- Agent 运行时的核心架构目标
  - 围绕会话(session)，支持状态持久化的可弹性系统(区别于基于请求-响应的互联网HTTP场景)
  - 灵活资源管理，文件、内容、cpu、gpu丰富资源的灵活和准确的计时机费


可观测部分的内容记录
- 可观测相比监控，跟强调能解释为什么，而不只是呈现一堆的数据
- AI可观测的关键能力项
  - 端到端全链路追踪(AI应用的端是从用户输入到最底层的大模型内部，而不同于传统应用底层是数据)
  - 全栈可观测(终端层、网关层、应用层、模型层多个维度)
  - 自动化评估，因为AI应用的不确定性，引入评估Agent来实现自动化评估(类似对抗的思路，但不会更加复杂了吗。绕一下，评估Agent能评估评估Agent吗？)
- 端到端全链路追踪的实现要点，面向AI应用的Trace语义(拆解AI应用的流程节点，比如 LLM、TOOL等节点)、支持多语言客户端的接入(python、java、go等)、标准化协议，兼容 OpenTelemetry W3C 协议，以实现跨语言组件的透传
- 推理引擎的可观测，推理引擎是运行LLM的软件，以开源的vLLM最常见。核心职责是 KVCache、GPU调度、分词调用LLM。根据其内部架构需要可以观测到如温度、topk、推理首token时间等细节内容。

评估部分的内容记录
- AI应用 VS 传统应用。 非确定性和概率性的 VS 确定性和规则性的。 相同输入不同输出 VS 相同输入相同输出。
- 幻觉，AI应用的阿克琉斯之踵，既是不可靠的根源，也是其创造力的源泉。
- 评估是AI治理的基石，评估体系应包含8个核心组件，性能、鲁棒与泛化、偏见与公平性、可解释性、合规性与伦理、持续监控、决策框架、自动化。(有点理论)
- LLM-as-a-Judge，基于 Agent 评估，解决人工评估的局限性。评估这个事情本身也可视为LLM的舒适区(海量、评判、可柔性)，因此有合理性。也要认识到Agent评估的局限性，Agent本身的偏见 和 LLM本身的能力上限。最好的方式还是人工和Agent评估同时使用，持续观察迭代。Agent评估亦有不同程度，可定义L0到L5。
- 阿里云提供的云原生评估方案，包括能力有 数据采集、预处理、实时评估、模板化。


AI安全部分的内容记录
- AI安全风险的来源，系统(AI底层软件带来的系统漏洞等)、网络(入侵、攻击等)、身份(Agent本身行为的管控)、数据(数据投毒，敏感信息)、模型(提示词攻击)、应用(入侵攻击等)
- 应用安全，新场景有 DoW (诱导AI持续消耗Token)。模型安全，新场景有 提示词注入(触发底层系统漏洞), 推理层攻击(模型伦理失控，比如google的go die事件，RAG知识库爬取)， 输出内容攻击(比如生成政府银行等政治、资产、权威性的内容)。身份安全，保护各种key、密钥的安全。其余的数据、网络安全等主要体现在云厂商内部，不做详述。

## 2、《AI原生应用架构白皮书》总结
有助于俯瞰AI应用的全景，理解如果要在阿里云部署AI应用，可以用到哪些组件，从云原生的角度，阿里云平台为AI应用提供了哪些能力。
浅层内容，快速扫一遍即可。

后续会继续分享用 Spring Alibaba AI 框架开发 Agent 的经验实践。