# 大家好，我是唐斩，这是第103天第120篇分享。

## kimi 的 askmeanything，8千字无AI手动整理
kimi k2 thinking 一经推出，以开源之姿屠榜人类最后考试，真正在国际上掀起关注，随后在 reddit 上开了一场 24h 的 AskMeAnything 有问必答活动，由创始人杨植麟带头和其他两位创始人一起马拉松回答任何问题。手动整理了所有kimi团队回答的问题，并附上自己的思考解读，原文地址
https://www.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/

问题集：
#1
Q：kimi下一代旗舰模型会用上KDA吗？
A：KDA性能和效率优于完整的MLA模型，K3很可能采用。
解读，KDA简单理解为更智能的混合专家模型，把固定的激活范围改成动态决定，性能更好，成本更低，更稳定。

#2
Q：K2会出 VL 版本吗？
A：在努力，敬请期待！
解读，VL是视觉-语言，有视觉能力的大语言模型。值得期待。可以识图理解并且组织语言回答。
多模态模型肯定也是必争之地，期待。

#3
Q：能否开发24G显存就能用的本地小模型？像qwen3的小模型就很受欢迎
A：收到🫡
解读，有很多喜欢用小模型的人，因为安全性，或者工作环境限制等原因，或者类似边缘计算的场景。小模型也非常有场景，只是现在公众的视野都被大模型盖住了。

#4
Q：k2不会像其他模型一样阿谀奉承，是故意的吗？
A：是的，整理数据时就有意设计如此。
解读，模型的风格和训练有非常强的关系，chatgpt4o的情感，deepseekr1的中文，甚至这周 gpt 专门升级一个 5.1 来让chatgpt更有情感。

#5
Q：kimi在专业写作方面的改进计划？ kimi审查更少人为干预更少但也在一些场景如残酷打斗场面会粉饰内容，反而造成不真实感。
A：没啥好说的，减少审查和人为干预是可行的，还要继续研究。
解读，众口难调，chatgpt要开放成人模式(更少限制和约束)，说明确实是用户需求所在，大厂商不去满足，总有去占领这块市场的。国外有of，ph，国内厂商肯定会有更严格的要求，模型厂商更谨慎的好。

#6
Q：除了30～48B尺寸外，还希望有100B左右的模型，适用于 AMD395 的机器。
A：会规划
解读，那就是一时半会都不会支持，没时间搞那么多尺寸。可能在个人电脑到满血部署之间，确实有一个适合小团队用的尺寸，可以供一般公司负担在本地使用，而不用担心安全和数据问题。

#7
Q：fp4对于int4的改进是否有重大意义，int4编码是否足够好了？
A：选int4主要是兼容非blackwellGPU，利用 int4推理的 marlin 内核。
关于 int4 知乎有kimi同事的详述，https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960
解读，阅读知乎内容，很清晰。简单说就是 用int4既能兼容更多显卡(考虑国内显卡落后的情况，以及增加k2的覆盖面)，以及团队的认知 int4 本身效果足够并且在RL阶段有一定优势，是一个好的架构balance选择。

#8
Q：何时 K3？
A：在 openai 万亿美元的数据中心建成之前。
解读，听着像玩梗，传达的意思是我们会搞快的，尽情期待！没查到具体openai数据中心的新闻，可能是之前openai和oracle等多方合作新闻的后续。能听出来kimi是把openai当对手看待的，很好。kimik2thinking的各种对比也是直接和gpt5比，非常有自信。

#9
Q：kimi for coding 基于API请求计费，贵！
A：用户可以看到明细，并且符合我们的成本结构，但是有道理会研究改进方法
解读，回答的很冠冕堂皇，但是几乎立刻就推出了 kimi for coding 的会员权益，每周1024次调用。
编码当然要包月用才爽啦😊。点赞👍。

#10
Q：kimik2thinking测试效果比gpt5好，但花费5到10倍时间，是否对输出效率有设计考虑？
A：有提升空间，努力中
解读，多想一会当然要慢一点。k2的api有turbo模式，输出会更快一些。

#11
Q：K2是最好的非思考模型，怎么做到的？希望 k2thinking也有详细技术报告
A：love & sweat，kimik2有详细技术报告 https://arxiv.org/pdf/2507.20534
无解读

#12
Q：是否会推出更重的型号？
A：情况危险时会考虑(dangerous)
无解读

#13
Q：K2-Instruct独特的创意写作，是特意的还是训练后的RL过程自然产生？
A：我们也喜欢它的写作风格，是后训练数据和评估的重要部分。
解读，没读懂回答，应该是特意训练的。

#14
Q：有没有小一点的32B或者20B的模型？
A：Kimi-Linear-48B-A3B-Instruct 是我们发布的小型模型之一，未来可能会训练更多模型。
解读，那就是没有，先用着 Kimi-Linear-48B-A3B-Instruct 吧

#15
Q：会考虑 Titans 这样的新架构吗？
A：Titan 数据集难以并行化，因此难以扩展。我们也希望与社区合作，开发性能更高、效率更高的测试时训练架构。
解读，titan架构师谷歌研究院提出的一种训练架构，是为训练超大规模大模型准备的。kimi言下之意是这玩意还不行，还得研究。 论文地址 https://arxiv.org/abs/2501.00663

#16
Q：kimi k2 instruct文笔独特且富有洞察，想知道其中秘诀。
A：预训练和后训练都会影响最终的训练效果。预训练编码相关的先验信息，而后训练则为其增添一些个性化的元素。观察不同的强化学习方案如何产生不同的效果，是一件非常有趣的事情。
解读，解答前面问题的疑问，是有意的，并且都有影响。

#17
Q：有没有类似 z.ai 的订阅方案
A：我们的 kimi.com 会员资格包含面向编码代理的 Kimi For Coding 订阅服务
解读， z.ai 就是智谱，国内最早有包月编程包的，首月20，次月开始40人民币

#18
Q：为什么用一个未经测试的优化器来训练如此大的模型
A：Muon 是一个未经其他厂商测试的优化器，但我们已经用它进行了所有的扩展测试，结果都通过了。我们对我们的研究成果充满信心。你或许认为 Muon 只是运气好，但实际上有几十种优化器和架构经不起这样的考验。
解读，Muon 是一种专门为神经网络 “隐藏层” 的二维参数设计的优化器。csdn的介绍文章 https://blog.csdn.net/kebijuelun/article/details/146072294
pytorch对其有具体实现，见文档 https://docs.pytorch.ac.cn/docs/stable/generated/torch.optim.Muon.html?utm_source=chatgpt.com
好的优化器能大幅提升训练效率，让模型往好的方向发展，能更快稳定下来。

#19
Q：个人很喜欢用kimi，并且公司用在测试阶段，但上线后就切到美国模型，担心封禁或者网络问题
A：封禁超过控制范围，但开源可以解决部分问题，企业可自行部署。kimi拥抱开源。
解读：老美也信创？ 企业线上使用多备份几个模型是正确的，但换模型会导致测试效果和上线效果不同，最好还是用相同模型，甚至相同的供应商。比如都用aws。

#20
Q：如何看待geimin openai过度赞扬用户的趋势？
A：模型有不同品味挺好的，多元化是趋势。
无解读

#21
Q：all in 纯文本Agent为了达到SOTA是短期权衡还是长期投资？
A：获得VL数据和训练需要时间，所以先发布文本模型。
解读，VL模型肯定也在规划中，原生多模态会是厂商的一个竞争点。

#22
Q：k2 thinking 460万美元的训练费用是真的吗？
A：这并非官方数据。训练成本很难量化，因为其中很大一部分用于研究和实验。
解读，不止，但具体多少我不说。 500万美元肯定是不够的，肯定要几千万美元的。

#23
Q：是否有计划开发 kimi线性规模的思考模型？
A：好建议，收到
解读，线性的优点是长上下文，计算效率高，内存占用低，扩展性好，部署友好。缺点是注意力效果会下降，不适合短文本，实现有复杂度，通用型不够好。适合特定场景。
代表性的模型是 MiniMax-01 ，https://www.minimax01.com/cn

#24
Q：Kimi K2 token消耗的多，后续有考虑优化吗？
A：确实，我们有限考虑的是性能而不是token效率，会加入奖励机制，以使模型学习到简化思考过程。
解读，先追求了极致效果，后续再优化用量。意味着，高价值的难题交给kimik2更合适。

#25
Q：为什么 Kimi k2 thinking 能单次推理如此长的时间和如此长的推理链，GPT5做不到。未来会提升推理速度吗？
A：推理时间取决于 API 吞吐量，token消耗量取决于模型的训练方式。我们训练 K2 Thinking 模型的方式倾向于使用相对更多的推理令牌以获得最佳结果。
我们的 Turbo API 速度应该会快得多。此外，K2 Thinking 本身就支持 INT4 类型，这进一步加快了推理过程。
解读，还是说当前追求的是思考结果所以token消耗高，推理时间是另外的优化方式，用turboapi会快一些。

#26
Q：对 deepseek-ocr 和 z.ai 的 glyphs 的看法，仅像素输入模型。是否考虑使用？
A：我个人认为这种方法过于刻意。我更倾向于继续探索特征空间，寻找更通用、与模态无关的方法来提高模型效率。
解读，不愿意用，认为是歧途。就像说自动驾驶做L2的公司就做不了L4一样，思维会被束缚反而无法正面突破难题。有点像那句，做正确的事而不是容易的事。

#27
Q：模型何时才能承认自己缺乏知识，而不是凭空捏造事实或数字？
A：说得对！理论上，这应该可以通过强化学习结合真实性奖励来解决。
解读，回答的很客气，有研究表明transforms架构的训练模式就必然会导致这个结果，RL也只是缓解，不可能彻底避免。彻底避免的方法也许在新的训练架构上，不追求补全，而是要求真实。

#28
Q：k2 thinking训练过程的最大挑战是什么？
A：其中一个挑战是支持“思考-工具-思考-工具”交错模式。这在LLM中是一种相对较新的行为，需要大量工作才能正确实现。
解读，“思考-工具-思考-工具”交错模式 和 ReACT 很像了，记得kimi是模型即Agent的支持者。我的看法是，模型即Agent确实有其优势，但现在模型的训练难度和成本，调整会比较慢。应用层还是需要更灵活的独立于模型外的可随时调整的Agent的。

#29
Q：喜欢 kimi k2 的性格设定，这种回应方式真的能提升跑分或其他方面的表现吗？
A：人们对这些细节的偏好各不相同。模特的风格通常反映了我们的喜好，很高兴你喜欢它！
无解读

#30
Q：你的AGI时间计划是什么样的？
A：AGI很难定义，更强大的模型即将问世。
解读，无话可说，等着吧。

#31
Q：我目前大量使用 sonnet4.5，因为支持葡萄牙语，但是太贵了，想换到开源模型。有计划支持100万上下文(1M)吗？法律AI需要更大的上下文。
A：之前尝试过1M上下文，当时服务成本太高，未来会考虑的。目前主要提升中文和英文性能。
解读，小语种还顾不上，先搞了中英文。百万上下文还用不起，等成本再降之后吧。非线形模型，上下文增加后的成本会平方级增加，而且对模型效果也会有影响，肯定不会很快支持的。

#32
Q：当 KDA、NSA 或 DSA 等重大理念仅被研究该架构的公司纳入其模型时，这通常是因为测试结果为负面，还是因为缺乏人力进行尝试？
A：坚持不懈地追求并实现某个目标需要付出巨大的努力，因此发明家在应用自己的想法时往往具有优势。话虽如此，我们也密切关注社区内的其他发明，并乐于尝试使用它们。
解读，没读懂问题，先原文展示吧。

#33
Q：K2 Thinking 发现了 sonnet4.5 和 opus4.1 几个月来多次评审都没解决的 bash 问题，这水平是新架构，还是训练数据质量提高的原因？
A：正确的评估方法和数据对性能至关重要。架构和优化器可以提高采样效率。
解读，综合原因，就是厉害了，放心使用。

#34
Q：长问题1、训练过程需要追踪的重要指标有哪些？2、应该在什么规模下测试架构变更看哪些指标？3、分享一些细节的技巧/资源？4、什么样的数据适合模型学习，哪些指标，怎么构建混合数据集？
A：1、关键指标 损失、基准和稳定性。2、渐进梯度实验，所有指标都至关重要。如果出现任何意外情况，我们会暂停扩展阶梯的推进过程，知道问题解决。3、最重要的超参数(hyperparameters)是学习率（以及学习率调度）。变量太多，所以最好先对超参数的分布情况有所了解，然后再深入研究超参数的搜索方法。4、好的数据集在训练过程中必须有良好的基准趋势。找到合适的数据集组合是一门艺术，一开始可以凭直觉判断，但最终要相信实验结果。
解读，专业解答，真正在做模型训练的应该有感觉，但似乎也不够深入。🤔

#35
Q：很快能体验到 kimi code 了吗？
A：可以。
解读，kimi有 kimi cli，https://www.kimi.com/coding/docs/en/kimi-cli.html

#36
Q：接下来还有哪些计划？
A：根据 https://www.moonshot.ai/ 的描述，我们的使命是“寻求将能量最佳转化为智能”，专注提升智能水平。
解读，无可奉告。按愿景来。

#37
Q：LLM 架构的下一个重大发展方向是什么？
A：对 Kimi Linear 进行了试验，结果看起来很有前景。它还可以与稀疏性结合使用
解读，看来 kimi-linear 模型有戏。

#38
Q：你们有计划提升模型的多语言功能吗？
A：我们很想教 Kimi 说更多语言，但我们掌握的语言种类和数量都有限。或许社区也能在这方面提供帮助，例如收集数据。
解读，看来还是没计划，也许可以依靠社区来驱动。

#39
Q：为什么k2 thinking 的推荐温度是1？是否有混合思维模式的规划？会与其他实验室合作吗？
A：GPT-5 和 Sonnet 4.5 在内的思维模型的标准值，也许和强化学习有关。在评估，但优先级不高。乐于与社区合作。
解读，强化学习的模型稳定通常都设置成1，可能的原因有避免输出退化1更稳定，保持奖励一致性还是1更稳定，经验规律 openai和anthropic 的论文里都推荐稳定是1。

#40
Q：在你当前的技术栈中，你最讨厌哪一种，却因为别无选择而不得不使用？
A：TensorBolard，收到很多投诉。虽然做了改进，但还是难以扩展，难以管理过多的实验，也难以显示准确的（未经降采样的）指标。
无解读

#41
Q：训练过程是否出现过天假的数据集对其他数据集产生了影响？
A：当数据集合并时，我们观察到的概括性结果会更好。
解读，没有这个问题，使劲加就行。

#42
Q：对嵌入模型感兴趣吗？例如用于检索、搜索的嵌入模型。
A：这些是 Agent用的工具。
解读，没兴趣。

#43
Q：为什么总参数数是 1 万亿，而不是 500 亿？为什么有效参数数是 32 亿，而不是 24 亿？关键在于总参数数、有效参数数，还是总参数数乘以有效参数数的平方根？
A：我们寻求在给定训练预算下接近最优的配置。稀疏度由经验扩展实验确定。更多细节可参考 K2 论文。
解读，几句话讲不清楚，看论文吧。

#44
Q：如果完全采用自主模式，在各领域会有多大进步？聊天模式下可以用了吗？
A：智能体模式即将推出，很可能会在《OK Computer》中上线。它将是完整的 K2 思维模式，比目前聊天模式下的功能更加强大。它将非常适合研究、编程以及其他智能体任务
解读，会出基于thinking的Agent模式，应该是接下来的重点工作。有点期待。

#45
Q：你们的训练平台硬件配置是怎样的？
A：我们使用配备 Infiniband 的 H800 GPU；它不如美国的高端 GPU 好，而且我们的显卡数量也处于劣势，但我们充分利用了每一张显卡！
解读，真不容易，期待国产芯片逆袭的时刻。

#46
Q：OAI 为什么会烧掉这么多钱？
A：不知道。只有山姆知道。我们有自己的方式和节奏。
无解读

#47
Q：我是一名学生，希望将来也能构建类似的模型。我应该从哪里开始呢？目前我已经完成了吴恩达的深度学习课程
A：从零开始构建一个小型LLM很有帮助，可以了解它的每一个组成部分。如果能研究一下纳米聊天工具就更好了。
解读，哈哈竟然直接推荐了 kapathy 的 nanochat。看来确实是适合上手的项目👍

#48
Q：介绍kimi k2 thinking的模型召回能力上的努力？长链推理的稳定性是如何保证的？未来是否有计划继续推出Kimi-Linear 模型？Kimi-K3未来可能取得突破的方向是什么？
A：我们使用端到端智能强化学习训练 K2 Thinking，这使得它在数百个步骤中能够调用工具，并且在包括检索在内的中间步骤上表现更佳。像 Kimi Linear 这样的小型模型很可爱，未来很可能会发布。我们非常希望在 K3 中融入重大的架构变革
解读，最浅显的说明就是效果依然主要来源于端到端智能强化训练。linear在研究，k3还无可奉告。

#49
Q：未来版本是否有计划增加上下文大小？256K 对于大型代码库来说并不算大。
A：我们应该能够在未来的版本中增加上下文长度。
解读，在做了。

#50
Q：文件上传的行为似乎在playground和api两者之间略有不同，是否计划合并API
A：会调查
解读，不知道，但听起来需要解决。

#51
Q：强化学习流程和数据是公司开放模型时所拥有的最后一点优势。但数据和流程的这一部分对社会具有巨大的价值
A：同意，发布安全对齐技术栈将使更多致力于进一步优化开放模型的人员受益。
解读，安全问题，是否是被我们长期低估的问题，等爆发的时候就晚了，有没有无名英雄正在这一领域战斗？

#52
Q：有没有计划推出更高阶的 TTC（比如像 GPT-5 Pro/3.0 那样的 DeepThink 竞品）
A：不是优先事项
解读，这里的 TTC 猜测是 Time-To-Completion 的意思，推理任务完成更快。

#53
Q：是否有计划与 Cerebras 或 Groq 合作？是否有计划开发双向对话模型？是否有计划推出更小尺寸的机型？是否有计划构建自己的编码命令行界面？
A：不发表评论。没有双向对话模型。任何双向注意力机制都可以用更长的因果注意力机制来实现。目前还没有针对 MacBook 友好机型的具体计划。编码命令行工具 https://github.com/MoonshotAI/kimi-cli
无解读

#54
Q：你们计划发布人工智能浏览器吗？
A：我们不需要创建另一个 Chromium 封装器来构建更好的模型。
解读，又捅Sam一刀，笑死😆。看来对 atlas 的评价不咋地。

