# 大家好，我是唐斩，这是第107天第124篇分享。

## 1、从 cloudflare 故障报告看事故复盘
cloudflare有多强，它挂了，导致半个互联网都挂了，工程师想问AI怎么了，发现ChatGPT挂了，想看看有什么新闻，发现x挂了，关机睡觉吧。
更强的是，cloudflare 第二天就发布了前一天的事故报告，值得学习，学习快速出报告的敏捷度，也学习报告内容的专业度。

报告原文，cn和en版
https://blog.cloudflare.com/zh-cn/18-november-2025-outage/
https://blog.cloudflare.com/18-november-2025-outage/


### 一个优秀的事故复盘只需要四部分内容。分别是
1、事故经过， 客观记录 时间，人物，事件。 比如xx点，运维，执行了数据库权限变更，比如xx点，客户，观察到网站流量异常
2、5why法挖掘事故根因。一步一步挖掘到最根本的不可再挖的原因。但事故不是一个问题引擎的，这一路上挖掘的过程就可以形成事故的原因链条。
3、彻底解决的方法。注意彻底解决一定不能依赖人，要依赖机制。
4、TODO事项。要落实到人，落实到截止时间。


### 看cloudflare的报告
1、梳理了清晰的过程表格，从操作开始，到出现影响，到开始排查，到找到问题核心，到问题解决，到影响彻底解决
定位2.5小时，解决1小时，彻底恢复2.5小时。这个效率你觉得如何？在你们公司复盘的话能过关吗？

2、解释原因，5why追问法，cloudflare用的洋葱法层层剥开。通过3个层级逐步解释
- 产品层面，报错页面，受影响的产品。
- 架构层面，指出问题在核心代理的bot管理部分，解释特征文件的工作流程和出错导致500原因，区分 FL 和 FL2 下的影响区别。
- 技术细节层面，clickhouse权限变更 -> 数据查询重复-> 超过限额 -> 报错代码行。

3、彻底解决方法，给的不算细致， 
- 直接解决，既然是文件问题，就加强它的监控。 
- 预防措施，全局终止开关。 能更快干预。
- 加速定位，清理报错。监控风暴是每个重大问题排查者的噩梦。
- 避免重复，审核代码解决类似问题。
4、TODO事项。内部会有，对外不用公布。


### 思考
1、没有绝对不出事故的系统，治理的目标应该是 缩短 MTTR (故障恢复时间) 和 增加 MTBF (故障间隔时间)。
2、缩短 MTTR 的方法是柔性架构，充足的预案和定时的演练。
3、增加 MTBF 的方法是工程质量，越往前的环节，效果越好。线上的问题，往前追溯，在设计阶段解决是做好的。 设计解决不了的，就是需求阶段的问题。
4、所有大问题都是一系列小问题堆叠而成，而小问题就是工程质量的疏忽造成的。
- 改权限的没想到权限穿透后查询数据会重复，否则要Review所有SQL。（db操作要附带可能影响SQL的报告）
- 写SQL的没想到会查出来那么多数据，否则要 distinct 和 limit。（所有sql加上database！）
- 限额的没想到真超额了会 panic error (从报告看是 FL2 的rust 代码，FL maybe 就不会 rust背锅重构代码背锅)
- FL2 rust 没想到超额了这行会报错，并且后面会500。 (java,go :你们不catch的吗
- 机器人模块 没想到 0分可能是出错了，不是真的机器人。

### 疑问
- 事故处理，最开始被误导成 ddos 是不是因为监控风暴，把报错那行信息淹没了？
- 事故处理，彻底恢复用了 2.5 小时，这个时间段在做什么？有什么缩短的方法？
- 事故处理，status是独立部署的，为什么也故障了，似乎没说明原因？
- 问题原因，FL2改用 Rust，FL是什么语言，两者重写的时候，健壮性设计是不是没考虑？
- 问题原因，灰度为什么没有发现问题？没有提及，不可能没有灰度吧？